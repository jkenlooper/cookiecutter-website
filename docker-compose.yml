version: '3.3'
services:
  web:
    ports:
      - "80:80"
      - "443:443"
    build:
      context: ./
      dockerfile: web.Dockerfile
      args:
        - conf=./web/web-nginx.production.conf
    image: nginx:1.12.2-alpine
    volumes:
      - type: volume
        source: root
        target: /srv/llama3-weboftomorrow-com/root
        volume:
          read_only: true
      - type: volume
        source: stats
        target: /www/stats
        volume:
          read_only: true
      - type: volume
        source: logs
        target: /www/logs
    networks:
      - llama3-weboftomorrow-com
    depends_on:
      - stats
      - api
  stats:
    build: ./stats/
    networks:
      - llama3-weboftomorrow-com
    volumes:
      - type: volume
        source: logs
        target: /var/log/nginx/
        volume:
          read_only: true
      - type: volume
        source: stats
        target: /usr/run/stats/www
      - type: volume
        source: statsdata
        target: /usr/run/stats/data
  chill:
    image: chill
    build:
      context: ./
      dockerfile: chill.Dockerfile
    command: serve
    env_file: .env
    networks:
      - llama3-weboftomorrow-com
    volumes:
      - type: volume
        source: root
        target: /usr/run/root
  api:
    build:
      context: ./
      dockerfile: api.Dockerfile
    networks:
      - llama3-weboftomorrow-com
    env_file: .env
    command: python src/api/app.py site.cfg
networks:
  llama3-weboftomorrow-com:
    driver: bridge
volumes:
  root:
  logs:
  stats:
  statsdata:
